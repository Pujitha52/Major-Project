{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Major_Project.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1fx6VVFTI193nh9mFdcvcpAHasXx6zyEF",
      "authorship_tag": "ABX9TyNpoTo2MAQluffKpBIwQMox",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Pujitha52/Major-Project/blob/main/Major_Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EiczMslnKxUf"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "df= '/content/drive/MyDrive/Smartknower-Machine Learning/Major Project/data.csv'\n",
        "df=pd.read_csv(df,encoding='latin-1',names=[\"Target\",\"Id\",\"Date\",\"Flag\",\"User\",\"Text\"])\n",
        "data=pd.DataFrame(df['Text'])\n",
        "trainData=data.head(85000)\n",
        "testData=data.tail(25000)"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pC9n5VjtLGZW",
        "outputId": "78634db0-6d71-421f-f67c-8629772fe4d6"
      },
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "\n",
        "stoplist = nltk.corpus.stopwords.words('english') \n",
        "stoplist.remove('no')\n",
        "stoplist.remove('not')\n",
        "stoplist"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['i',\n",
              " 'me',\n",
              " 'my',\n",
              " 'myself',\n",
              " 'we',\n",
              " 'our',\n",
              " 'ours',\n",
              " 'ourselves',\n",
              " 'you',\n",
              " \"you're\",\n",
              " \"you've\",\n",
              " \"you'll\",\n",
              " \"you'd\",\n",
              " 'your',\n",
              " 'yours',\n",
              " 'yourself',\n",
              " 'yourselves',\n",
              " 'he',\n",
              " 'him',\n",
              " 'his',\n",
              " 'himself',\n",
              " 'she',\n",
              " \"she's\",\n",
              " 'her',\n",
              " 'hers',\n",
              " 'herself',\n",
              " 'it',\n",
              " \"it's\",\n",
              " 'its',\n",
              " 'itself',\n",
              " 'they',\n",
              " 'them',\n",
              " 'their',\n",
              " 'theirs',\n",
              " 'themselves',\n",
              " 'what',\n",
              " 'which',\n",
              " 'who',\n",
              " 'whom',\n",
              " 'this',\n",
              " 'that',\n",
              " \"that'll\",\n",
              " 'these',\n",
              " 'those',\n",
              " 'am',\n",
              " 'is',\n",
              " 'are',\n",
              " 'was',\n",
              " 'were',\n",
              " 'be',\n",
              " 'been',\n",
              " 'being',\n",
              " 'have',\n",
              " 'has',\n",
              " 'had',\n",
              " 'having',\n",
              " 'do',\n",
              " 'does',\n",
              " 'did',\n",
              " 'doing',\n",
              " 'a',\n",
              " 'an',\n",
              " 'the',\n",
              " 'and',\n",
              " 'but',\n",
              " 'if',\n",
              " 'or',\n",
              " 'because',\n",
              " 'as',\n",
              " 'until',\n",
              " 'while',\n",
              " 'of',\n",
              " 'at',\n",
              " 'by',\n",
              " 'for',\n",
              " 'with',\n",
              " 'about',\n",
              " 'against',\n",
              " 'between',\n",
              " 'into',\n",
              " 'through',\n",
              " 'during',\n",
              " 'before',\n",
              " 'after',\n",
              " 'above',\n",
              " 'below',\n",
              " 'to',\n",
              " 'from',\n",
              " 'up',\n",
              " 'down',\n",
              " 'in',\n",
              " 'out',\n",
              " 'on',\n",
              " 'off',\n",
              " 'over',\n",
              " 'under',\n",
              " 'again',\n",
              " 'further',\n",
              " 'then',\n",
              " 'once',\n",
              " 'here',\n",
              " 'there',\n",
              " 'when',\n",
              " 'where',\n",
              " 'why',\n",
              " 'how',\n",
              " 'all',\n",
              " 'any',\n",
              " 'both',\n",
              " 'each',\n",
              " 'few',\n",
              " 'more',\n",
              " 'most',\n",
              " 'other',\n",
              " 'some',\n",
              " 'such',\n",
              " 'nor',\n",
              " 'only',\n",
              " 'own',\n",
              " 'same',\n",
              " 'so',\n",
              " 'than',\n",
              " 'too',\n",
              " 'very',\n",
              " 's',\n",
              " 't',\n",
              " 'can',\n",
              " 'will',\n",
              " 'just',\n",
              " 'don',\n",
              " \"don't\",\n",
              " 'should',\n",
              " \"should've\",\n",
              " 'now',\n",
              " 'd',\n",
              " 'll',\n",
              " 'm',\n",
              " 'o',\n",
              " 're',\n",
              " 've',\n",
              " 'y',\n",
              " 'ain',\n",
              " 'aren',\n",
              " \"aren't\",\n",
              " 'couldn',\n",
              " \"couldn't\",\n",
              " 'didn',\n",
              " \"didn't\",\n",
              " 'doesn',\n",
              " \"doesn't\",\n",
              " 'hadn',\n",
              " \"hadn't\",\n",
              " 'hasn',\n",
              " \"hasn't\",\n",
              " 'haven',\n",
              " \"haven't\",\n",
              " 'isn',\n",
              " \"isn't\",\n",
              " 'ma',\n",
              " 'mightn',\n",
              " \"mightn't\",\n",
              " 'mustn',\n",
              " \"mustn't\",\n",
              " 'needn',\n",
              " \"needn't\",\n",
              " 'shan',\n",
              " \"shan't\",\n",
              " 'shouldn',\n",
              " \"shouldn't\",\n",
              " 'wasn',\n",
              " \"wasn't\",\n",
              " 'weren',\n",
              " \"weren't\",\n",
              " 'won',\n",
              " \"won't\",\n",
              " 'wouldn',\n",
              " \"wouldn't\"]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5KF0yE1YLV-b",
        "outputId": "b41cb9a6-8112-41d7-e46a-9c05dfa6fc5b"
      },
      "source": [
        "##Preprocessing data\n",
        "\n",
        "\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "import string\n",
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.tokenize.toktok import ToktokTokenizer\n",
        "tokenizer= ToktokTokenizer()\n",
        "\n",
        "# Remove numbers\n",
        "def remove_numbers(text):\n",
        "\tresult = re.sub(r'\\d+', '', text)\n",
        "\treturn result\n",
        "\n",
        "# remove punctuation\n",
        "def remove_punctuation(text):\n",
        "\ttranslator = str.maketrans('', '', string.punctuation)\n",
        "\treturn text.translate(translator)\n",
        "\n",
        "#lowercase\n",
        "def text_lowercase(text):\n",
        "\treturn text.lower()\n",
        "\n",
        "# remove whitespace from text\n",
        "def remove_whitespace(text):\n",
        "\treturn \" \".join(text.split())\n",
        "\n",
        "\n",
        "\n",
        "def remove_stopwords(text):\n",
        "    tokens = tokenizer.tokenize(text)\n",
        "    tokens = [token.strip() for token in tokens]\n",
        "    fil_tokens = [token for token in tokens if token not in stoplist]\n",
        "    fil_text = ' '.join(fil_tokens)\n",
        "    return fil_text\n",
        "\n",
        "trainData.Text=trainData.Text.apply(lambda x:x.lower())\n",
        "trainData.Text=trainData.Text.apply(remove_numbers)\n",
        "trainData.Text=trainData.Text.apply(remove_punctuation)\n",
        "trainData.Text=trainData.Text.apply(remove_whitespace)\n",
        "trainData.Text=trainData.Text.apply(remove_stopwords)\n",
        "\n",
        "\n",
        "testData.Text=testData.Text.apply(lambda x:x.lower())\n",
        "testData.Text=testData.Text.apply(remove_numbers)\n",
        "testData.Text=testData.Text.apply(remove_punctuation)\n",
        "testData.Text=testData.Text.apply(remove_whitespace)\n",
        "testData.Text=testData.Text.apply(remove_stopwords)\n",
        "\n",
        "print(trainData.head(5))\n",
        "print(testData.head(5))"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/pandas/core/generic.py:5170: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  self[name] = value\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "                                                Text\n",
            "0  switchfoot httptwitpiccomyzl awww thats bummer...\n",
            "1  upset cant update facebook texting might cry r...\n",
            "2  kenichan dived many times ball managed save re...\n",
            "3                   whole body feels itchy like fire\n",
            "4    nationwideclass no not behaving im mad cant see\n",
            "                                                      Text\n",
            "1575000    orianthi never heard sounds really yummy though\n",
            "1575001  lizzybee yeah itll twilight choice awards vote...\n",
            "1575002   httptwitpiccomjmx took look leather thats wassup\n",
            "1575003  warm doublebreasted cardigan nice thanx honey ...\n",
            "1575004         skype account alexarl en un mesesito chile\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v6_ERd67Lo9w",
        "outputId": "d30fad27-9cc8-43a6-cfc9-7e125771581d"
      },
      "source": [
        "!pip install vaderSentiment"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting vaderSentiment\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/76/fc/310e16254683c1ed35eeb97386986d6c00bc29df17ce280aed64d55537e9/vaderSentiment-3.3.2-py2.py3-none-any.whl (125kB)\n",
            "\r\u001b[K     |██▋                             | 10kB 13.8MB/s eta 0:00:01\r\u001b[K     |█████▏                          | 20kB 9.5MB/s eta 0:00:01\r\u001b[K     |███████▉                        | 30kB 8.1MB/s eta 0:00:01\r\u001b[K     |██████████▍                     | 40kB 6.2MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 51kB 3.3MB/s eta 0:00:01\r\u001b[K     |███████████████▋                | 61kB 3.5MB/s eta 0:00:01\r\u001b[K     |██████████████████▏             | 71kB 3.6MB/s eta 0:00:01\r\u001b[K     |████████████████████▉           | 81kB 3.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████▍        | 92kB 3.9MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 102kB 3.2MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▋   | 112kB 3.2MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▏| 122kB 3.2MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 133kB 3.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from vaderSentiment) (2.23.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->vaderSentiment) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->vaderSentiment) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->vaderSentiment) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->vaderSentiment) (2021.5.30)\n",
            "Installing collected packages: vaderSentiment\n",
            "Successfully installed vaderSentiment-3.3.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0YkMnhEgMwFq"
      },
      "source": [
        "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
        "vs= SentimentIntensityAnalyzer()\n",
        "trainData['compound'] = trainData['Text'].apply(lambda x: vs.polarity_scores(x)['compound'])\n",
        "testData['compound'] = testData['Text'].apply(lambda x: vs.polarity_scores(x)['compound'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "KH4Oh1JLNIDI",
        "outputId": "dafb9e88-724d-4142-a94a-8ad0d18e00b0"
      },
      "source": [
        "trainData.sample(frac=1).head(5)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Text</th>\n",
              "      <th>compound</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>71476</th>\n",
              "      <td>realbillbailey morning bill morning weather ne...</td>\n",
              "      <td>-0.0790</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11290</th>\n",
              "      <td>everyone else getting tons subscribers work ha...</td>\n",
              "      <td>0.2732</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17474</th>\n",
              "      <td>talktolizzy friend took nick twitter crying lol</td>\n",
              "      <td>0.4404</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>79706</th>\n",
              "      <td>today super awesome taking cosplay pics long n...</td>\n",
              "      <td>0.8402</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23611</th>\n",
              "      <td>think time revise lessons exams</td>\n",
              "      <td>0.0000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                    Text  compound\n",
              "71476  realbillbailey morning bill morning weather ne...   -0.0790\n",
              "11290  everyone else getting tons subscribers work ha...    0.2732\n",
              "17474    talktolizzy friend took nick twitter crying lol    0.4404\n",
              "79706  today super awesome taking cosplay pics long n...    0.8402\n",
              "23611                    think time revise lessons exams    0.0000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "otq3CLMnWJnw"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "trainData['compound'] = np.where((trainData['compound'] >= 0.5), '1', \n",
        "                          np.where((trainData['compound'] <= -0.5), '-1', '0'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "09B4kJItZEsq",
        "outputId": "bc53103a-d7bb-4a10-a66f-24af64088ba2"
      },
      "source": [
        "trainData.head(5)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Text</th>\n",
              "      <th>compound</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>switchfoot httptwitpiccomyzl awww thats bummer...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>upset cant update facebook texting might cry r...</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>kenichan dived many times ball managed save re...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>whole body feels itchy like fire</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>nationwideclass no not behaving im mad cant see</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                Text compound\n",
              "0  switchfoot httptwitpiccomyzl awww thats bummer...        0\n",
              "1  upset cant update facebook texting might cry r...       -1\n",
              "2  kenichan dived many times ball managed save re...        0\n",
              "3                   whole body feels itchy like fire        0\n",
              "4    nationwideclass no not behaving im mad cant see        0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fXoyGQbpbd7W"
      },
      "source": [
        "testData['compound'] = np.where((testData['compound'] >= 0.5), '1', \n",
        "                          np.where((testData['compound'] <= -0.5), '-1', '0'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "FkoYrS0ZbiNV",
        "outputId": "7e6504ed-55b5-4e3b-d620-0d87c34acf36"
      },
      "source": [
        "testData.head()"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Text</th>\n",
              "      <th>compound</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1575000</th>\n",
              "      <td>orianthi never heard sounds really yummy though</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1575001</th>\n",
              "      <td>lizzybee yeah itll twilight choice awards vote...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1575002</th>\n",
              "      <td>httptwitpiccomjmx took look leather thats wassup</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1575003</th>\n",
              "      <td>warm doublebreasted cardigan nice thanx honey ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1575004</th>\n",
              "      <td>skype account alexarl en un mesesito chile</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                      Text compound\n",
              "1575000    orianthi never heard sounds really yummy though        1\n",
              "1575001  lizzybee yeah itll twilight choice awards vote...        1\n",
              "1575002   httptwitpiccomjmx took look leather thats wassup        0\n",
              "1575003  warm doublebreasted cardigan nice thanx honey ...        1\n",
              "1575004         skype account alexarl en un mesesito chile        0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bl69Lptmbo05"
      },
      "source": [
        "#Fitting Model\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "# Create feature vectors\n",
        "vectorizer = TfidfVectorizer(min_df = 5,\n",
        "                             max_df = 0.8,\n",
        "                             sublinear_tf = True,\n",
        "                             use_idf = True)\n",
        "train_vectors = vectorizer.fit_transform(trainData['Text'].values.astype('U'))\n",
        "test_vectors = vectorizer.transform(testData['Text'].values.astype('U'))"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5qVfuFgmb0zl",
        "outputId": "55ebb1b1-1d93-40ff-a625-43808b050ea3"
      },
      "source": [
        "import time\n",
        "from sklearn import svm\n",
        "from sklearn.metrics import classification_report\n",
        "# Perform classification with SVM, kernel=linear\n",
        "classifier_linear = svm.SVC(kernel='linear')\n",
        "t0 = time.time()\n",
        "classifier_linear.fit(train_vectors, trainData['compound'])\n",
        "t1 = time.time()\n",
        "prediction_linear = classifier_linear.predict(test_vectors)\n",
        "t2 = time.time()\n",
        "time_linear_train = t1-t0\n",
        "time_linear_predict = t2-t1\n",
        "# results\n",
        "print(\"Training time: %fs; Prediction time: %fs\" % (time_linear_train, time_linear_predict))\n",
        "report = classification_report(testData['compound'], prediction_linear, output_dict=True)\n",
        "print('positive: ', report['1'])\n",
        "print('negative: ', report['-1'])\n",
        "print('neutral: ',report['0'])"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training time: 797.735205s; Prediction time: 70.839121s\n",
            "positive:  {'precision': 0.9019192987758803, 'recall': 0.7051039697542533, 'f1-score': 0.7914594522909622, 'support': 8464}\n",
            "negative:  {'precision': 0.5790408525754884, 'recall': 0.44474761255115963, 'f1-score': 0.5030864197530864, 'support': 733}\n",
            "neutral:  {'precision': 0.8410774410774411, 'recall': 0.9484275137632096, 'f1-score': 0.8915325818636053, 'support': 15803}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8X8yLa5TcMju",
        "outputId": "cbcc62c5-d8f7-40ba-f4c4-5668da4a2384"
      },
      "source": [
        "review = \"\"\"I hate you\"\"\"\n",
        "review_vector = vectorizer.transform([review]) # vectorizing\n",
        "print(classifier_linear.predict(review_vector))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['-1']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EZpR8W6bsN2G"
      },
      "source": [
        "from sklearn.metrics import  accuracy_score,confusion_matrix"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cS1u_M3Gshd6",
        "outputId": "00f61bdf-eb2c-437a-de33-714f55eda53b"
      },
      "source": [
        "accuracy_score(testData['compound'],prediction_linear)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.85128"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jV9luFfFsofP",
        "outputId": "53474d00-d63b-4fb7-955b-b3294e797796"
      },
      "source": [
        "confusion_matrix(testData['compound'],prediction_linear)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[  326,   373,    34],\n",
              "       [  200, 14988,   615],\n",
              "       [   37,  2459,  5968]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eTqH_-dVszOJ"
      },
      "source": [
        "#saving the model\n",
        "\n",
        "import pickle\n",
        "# pickling the vectorizer\n",
        "pickle.dump(vectorizer, open('vectorizer.sav', 'wb'))\n",
        "# pickling the model\n",
        "pickle.dump(classifier_linear, open('classifier.sav', 'wb'))"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yH6m1HY2Ddse"
      },
      "source": [
        "#streamlit app\n",
        "\n",
        "!pip install streamlit --quiet\n",
        "!pip install pyngrok==4.1.1 --quiet\n",
        "from pyngrok import ngrok"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bpWZkB8xE-4A",
        "outputId": "f2951c9a-962f-40ea-caac-5a989e01db9e"
      },
      "source": [
        "%%writefile app.py\n",
        "import streamlit as st\n",
        "import pickle\n",
        "from sklearn import svm\n",
        "\n",
        "\n",
        "filename = 'classifier.sav'\n",
        "classifier_linear=pickle.load(open(filename,'rb'))\n",
        "filename1 = 'vectorizer.sav'\n",
        "vectorizer=pickle.load(open(filename1,'rb'))\n",
        "\n",
        "st.title('Sentiment Analysis using Machine Learning Algorithms')\n",
        "text=st.text_area('Enter text')\n",
        "\n",
        "st.set_option('deprecation.showfileUploaderEncoding', False)\n",
        "\n",
        "if st.button(\"Predict\"):\n",
        "  review_vector = vectorizer.transform([text])\n",
        "  x=classifier_linear.predict(review_vector)\n",
        "  if (x[0]=='0'):\n",
        "    st.success('Text is  NEUTRAL')\n",
        "  if (x[0]=='1'):\n",
        "    st.success('Text is POSITIVE')\n",
        "  if (x[0]=='-1'):\n",
        "    st.success(' Text is NEGATIVE')"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Writing app.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I5CW6aiOFEw5",
        "outputId": "5ec255d8-8f24-467c-beec-07144072250f"
      },
      "source": [
        "!nohup streamlit run app.py &\n",
        "url = ngrok.connect(port = '8501')\n",
        "print(url)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "nohup: appending output to 'nohup.out'\n",
            "http://c72eab29a853.ngrok.io\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KUbT-XqJvd1_",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "c274b18b-6a2d-42a2-eedf-e9f5f60f2507"
      },
      "source": [
        "import sys\n",
        "sys.version"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'3.7.10 (default, May  3 2021, 02:48:31) \\n[GCC 7.5.0]'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JpsbWWhoV8AP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b8cf53f3-9ef9-45a1-9099-3a14c68f0f7a"
      },
      "source": [
        "import pickle \n",
        "print(pickle.format_version)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "4.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wNckDzscc2Yw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7fa69f4f-4901-41b3-c6ca-fcd18e580c23"
      },
      "source": [
        "import sklearn\n",
        "print(sklearn.__version__)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.22.2.post1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KauPf9PIlUYl"
      },
      "source": [
        "!pip freeze > requirements.txt"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "82TTgQ4VzIRz"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}