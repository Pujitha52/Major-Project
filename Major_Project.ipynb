{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Major_Project.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1fx6VVFTI193nh9mFdcvcpAHasXx6zyEF",
      "authorship_tag": "ABX9TyNFhw056DylBFakDl71mjz/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Pujitha52/Major-Project/blob/main/Major_Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EiczMslnKxUf"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "df= '/content/drive/MyDrive/Smartknower-Machine Learning/Major Project/data.csv'\n",
        "df=pd.read_csv(df,encoding='latin-1',names=[\"Target\",\"Id\",\"Date\",\"Flag\",\"User\",\"Text\"])\n",
        "data=pd.DataFrame(df['Text'])\n",
        "trainData=data.head(85000)\n",
        "testData=data.tail(25000)"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pC9n5VjtLGZW",
        "outputId": "88c909c0-038a-43f1-a188-b2380a36ef8c"
      },
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "\n",
        "stoplist = nltk.corpus.stopwords.words('english') \n",
        "stoplist.remove('no')\n",
        "stoplist.remove('not')\n",
        "stoplist"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['i',\n",
              " 'me',\n",
              " 'my',\n",
              " 'myself',\n",
              " 'we',\n",
              " 'our',\n",
              " 'ours',\n",
              " 'ourselves',\n",
              " 'you',\n",
              " \"you're\",\n",
              " \"you've\",\n",
              " \"you'll\",\n",
              " \"you'd\",\n",
              " 'your',\n",
              " 'yours',\n",
              " 'yourself',\n",
              " 'yourselves',\n",
              " 'he',\n",
              " 'him',\n",
              " 'his',\n",
              " 'himself',\n",
              " 'she',\n",
              " \"she's\",\n",
              " 'her',\n",
              " 'hers',\n",
              " 'herself',\n",
              " 'it',\n",
              " \"it's\",\n",
              " 'its',\n",
              " 'itself',\n",
              " 'they',\n",
              " 'them',\n",
              " 'their',\n",
              " 'theirs',\n",
              " 'themselves',\n",
              " 'what',\n",
              " 'which',\n",
              " 'who',\n",
              " 'whom',\n",
              " 'this',\n",
              " 'that',\n",
              " \"that'll\",\n",
              " 'these',\n",
              " 'those',\n",
              " 'am',\n",
              " 'is',\n",
              " 'are',\n",
              " 'was',\n",
              " 'were',\n",
              " 'be',\n",
              " 'been',\n",
              " 'being',\n",
              " 'have',\n",
              " 'has',\n",
              " 'had',\n",
              " 'having',\n",
              " 'do',\n",
              " 'does',\n",
              " 'did',\n",
              " 'doing',\n",
              " 'a',\n",
              " 'an',\n",
              " 'the',\n",
              " 'and',\n",
              " 'but',\n",
              " 'if',\n",
              " 'or',\n",
              " 'because',\n",
              " 'as',\n",
              " 'until',\n",
              " 'while',\n",
              " 'of',\n",
              " 'at',\n",
              " 'by',\n",
              " 'for',\n",
              " 'with',\n",
              " 'about',\n",
              " 'against',\n",
              " 'between',\n",
              " 'into',\n",
              " 'through',\n",
              " 'during',\n",
              " 'before',\n",
              " 'after',\n",
              " 'above',\n",
              " 'below',\n",
              " 'to',\n",
              " 'from',\n",
              " 'up',\n",
              " 'down',\n",
              " 'in',\n",
              " 'out',\n",
              " 'on',\n",
              " 'off',\n",
              " 'over',\n",
              " 'under',\n",
              " 'again',\n",
              " 'further',\n",
              " 'then',\n",
              " 'once',\n",
              " 'here',\n",
              " 'there',\n",
              " 'when',\n",
              " 'where',\n",
              " 'why',\n",
              " 'how',\n",
              " 'all',\n",
              " 'any',\n",
              " 'both',\n",
              " 'each',\n",
              " 'few',\n",
              " 'more',\n",
              " 'most',\n",
              " 'other',\n",
              " 'some',\n",
              " 'such',\n",
              " 'nor',\n",
              " 'only',\n",
              " 'own',\n",
              " 'same',\n",
              " 'so',\n",
              " 'than',\n",
              " 'too',\n",
              " 'very',\n",
              " 's',\n",
              " 't',\n",
              " 'can',\n",
              " 'will',\n",
              " 'just',\n",
              " 'don',\n",
              " \"don't\",\n",
              " 'should',\n",
              " \"should've\",\n",
              " 'now',\n",
              " 'd',\n",
              " 'll',\n",
              " 'm',\n",
              " 'o',\n",
              " 're',\n",
              " 've',\n",
              " 'y',\n",
              " 'ain',\n",
              " 'aren',\n",
              " \"aren't\",\n",
              " 'couldn',\n",
              " \"couldn't\",\n",
              " 'didn',\n",
              " \"didn't\",\n",
              " 'doesn',\n",
              " \"doesn't\",\n",
              " 'hadn',\n",
              " \"hadn't\",\n",
              " 'hasn',\n",
              " \"hasn't\",\n",
              " 'haven',\n",
              " \"haven't\",\n",
              " 'isn',\n",
              " \"isn't\",\n",
              " 'ma',\n",
              " 'mightn',\n",
              " \"mightn't\",\n",
              " 'mustn',\n",
              " \"mustn't\",\n",
              " 'needn',\n",
              " \"needn't\",\n",
              " 'shan',\n",
              " \"shan't\",\n",
              " 'shouldn',\n",
              " \"shouldn't\",\n",
              " 'wasn',\n",
              " \"wasn't\",\n",
              " 'weren',\n",
              " \"weren't\",\n",
              " 'won',\n",
              " \"won't\",\n",
              " 'wouldn',\n",
              " \"wouldn't\"]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5KF0yE1YLV-b",
        "outputId": "e64f7c9e-2a60-4bdc-858c-1112b1d2ef21"
      },
      "source": [
        "##Preprocessing data\n",
        "\n",
        "\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "import string\n",
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.tokenize.toktok import ToktokTokenizer\n",
        "tokenizer= ToktokTokenizer()\n",
        "\n",
        "# Remove numbers\n",
        "def remove_numbers(text):\n",
        "\tresult = re.sub(r'\\d+', '', text)\n",
        "\treturn result\n",
        "\n",
        "# remove punctuation\n",
        "def remove_punctuation(text):\n",
        "\ttranslator = str.maketrans('', '', string.punctuation)\n",
        "\treturn text.translate(translator)\n",
        "\n",
        "#lowercase\n",
        "def text_lowercase(text):\n",
        "\treturn text.lower()\n",
        "\n",
        "# remove whitespace from text\n",
        "def remove_whitespace(text):\n",
        "\treturn \" \".join(text.split())\n",
        "\n",
        "\n",
        "\n",
        "def remove_stopwords(text):\n",
        "    tokens = tokenizer.tokenize(text)\n",
        "    tokens = [token.strip() for token in tokens]\n",
        "    fil_tokens = [token for token in tokens if token not in stoplist]\n",
        "    fil_text = ' '.join(fil_tokens)\n",
        "    return fil_text\n",
        "\n",
        "trainData.Text=trainData.Text.apply(lambda x:x.lower())\n",
        "trainData.Text=trainData.Text.apply(remove_numbers)\n",
        "trainData.Text=trainData.Text.apply(remove_punctuation)\n",
        "trainData.Text=trainData.Text.apply(remove_whitespace)\n",
        "trainData.Text=trainData.Text.apply(remove_stopwords)\n",
        "\n",
        "\n",
        "testData.Text=testData.Text.apply(lambda x:x.lower())\n",
        "testData.Text=testData.Text.apply(remove_numbers)\n",
        "testData.Text=testData.Text.apply(remove_punctuation)\n",
        "testData.Text=testData.Text.apply(remove_whitespace)\n",
        "testData.Text=testData.Text.apply(remove_stopwords)\n",
        "\n",
        "print(trainData.head(5))\n",
        "print(testData.head(5))"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/pandas/core/generic.py:5170: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  self[name] = value\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "                                                Text\n",
            "0  switchfoot httptwitpiccomyzl awww thats bummer...\n",
            "1  upset cant update facebook texting might cry r...\n",
            "2  kenichan dived many times ball managed save re...\n",
            "3                   whole body feels itchy like fire\n",
            "4    nationwideclass no not behaving im mad cant see\n",
            "                                                      Text\n",
            "1575000    orianthi never heard sounds really yummy though\n",
            "1575001  lizzybee yeah itll twilight choice awards vote...\n",
            "1575002   httptwitpiccomjmx took look leather thats wassup\n",
            "1575003  warm doublebreasted cardigan nice thanx honey ...\n",
            "1575004         skype account alexarl en un mesesito chile\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v6_ERd67Lo9w",
        "outputId": "ba05686c-fea3-427f-9b30-14395a9340d3"
      },
      "source": [
        "!pip install vaderSentiment"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: vaderSentiment in /usr/local/lib/python3.7/dist-packages (3.3.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from vaderSentiment) (2.23.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->vaderSentiment) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->vaderSentiment) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->vaderSentiment) (2021.5.30)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->vaderSentiment) (2.10)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0YkMnhEgMwFq"
      },
      "source": [
        "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
        "vs= SentimentIntensityAnalyzer()\n",
        "trainData['compound'] = trainData['Text'].apply(lambda x: vs.polarity_scores(x)['compound'])\n",
        "testData['compound'] = testData['Text'].apply(lambda x: vs.polarity_scores(x)['compound'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 203
        },
        "id": "KH4Oh1JLNIDI",
        "outputId": "5c3d0178-8813-43d5-b951-670d262970cd"
      },
      "source": [
        "trainData.sample(frac=1).head(5)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Text</th>\n",
              "      <th>compound</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>496</th>\n",
              "      <td>wanna punk rock band</td>\n",
              "      <td>0.0000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20475</th>\n",
              "      <td>dammit want hear nic chagalls beautiful riff asot</td>\n",
              "      <td>0.6369</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>65844</th>\n",
              "      <td>tinkcashley ooo u bring back</td>\n",
              "      <td>0.0000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>67567</th>\n",
              "      <td>im sad tonight quottough lovequot season final...</td>\n",
              "      <td>-0.4767</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>62407</th>\n",
              "      <td>nipplelicious pics im sure missed something sorry</td>\n",
              "      <td>-0.0516</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                    Text  compound\n",
              "496                                 wanna punk rock band    0.0000\n",
              "20475  dammit want hear nic chagalls beautiful riff asot    0.6369\n",
              "65844                       tinkcashley ooo u bring back    0.0000\n",
              "67567  im sad tonight quottough lovequot season final...   -0.4767\n",
              "62407  nipplelicious pics im sure missed something sorry   -0.0516"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "otq3CLMnWJnw"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "trainData['compound'] = np.where((trainData['compound'] >= 0.5), '1', \n",
        "                          np.where((trainData['compound'] <= -0.5), '-1', '0'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 203
        },
        "id": "09B4kJItZEsq",
        "outputId": "5479590e-6e4d-40f3-9d67-de385b40e7b5"
      },
      "source": [
        "trainData.head(5)"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Text</th>\n",
              "      <th>compound</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>switchfoot httptwitpiccomyzl awww thats bummer...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>upset cant update facebook texting might cry r...</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>kenichan dived many times ball managed save re...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>whole body feels itchy like fire</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>nationwideclass no not behaving im mad cant see</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                Text compound\n",
              "0  switchfoot httptwitpiccomyzl awww thats bummer...        0\n",
              "1  upset cant update facebook texting might cry r...       -1\n",
              "2  kenichan dived many times ball managed save re...        0\n",
              "3                   whole body feels itchy like fire        0\n",
              "4    nationwideclass no not behaving im mad cant see        0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fXoyGQbpbd7W"
      },
      "source": [
        "testData['compound'] = np.where((testData['compound'] >= 0.5), '1', \n",
        "                          np.where((testData['compound'] <= -0.5), '-1', '0'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 203
        },
        "id": "FkoYrS0ZbiNV",
        "outputId": "1d91786b-753d-4d3b-fb7d-2e6cf4bd2389"
      },
      "source": [
        "testData.head()"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Text</th>\n",
              "      <th>compound</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1575000</th>\n",
              "      <td>orianthi never heard sounds really yummy though</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1575001</th>\n",
              "      <td>lizzybee yeah itll twilight choice awards vote...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1575002</th>\n",
              "      <td>httptwitpiccomjmx took look leather thats wassup</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1575003</th>\n",
              "      <td>warm doublebreasted cardigan nice thanx honey ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1575004</th>\n",
              "      <td>skype account alexarl en un mesesito chile</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                      Text compound\n",
              "1575000    orianthi never heard sounds really yummy though        1\n",
              "1575001  lizzybee yeah itll twilight choice awards vote...        1\n",
              "1575002   httptwitpiccomjmx took look leather thats wassup        0\n",
              "1575003  warm doublebreasted cardigan nice thanx honey ...        1\n",
              "1575004         skype account alexarl en un mesesito chile        0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bl69Lptmbo05"
      },
      "source": [
        "#Fitting Model\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "# Create feature vectors\n",
        "vectorizer = TfidfVectorizer(min_df = 5,\n",
        "                             max_df = 0.8,\n",
        "                             sublinear_tf = True,\n",
        "                             use_idf = True)\n",
        "train_vectors = vectorizer.fit_transform(trainData['Text'].values.astype('U'))\n",
        "test_vectors = vectorizer.transform(testData['Text'].values.astype('U'))"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5qVfuFgmb0zl",
        "outputId": "47d9da9a-1900-4729-9c16-8f45a168db5e"
      },
      "source": [
        "import time\n",
        "from sklearn import svm\n",
        "from sklearn.metrics import classification_report\n",
        "# Perform classification with SVM, kernel=linear\n",
        "classifier_linear = svm.SVC(kernel='linear')\n",
        "t0 = time.time()\n",
        "classifier_linear.fit(train_vectors, trainData['compound'])\n",
        "t1 = time.time()\n",
        "prediction_linear = classifier_linear.predict(test_vectors)\n",
        "t2 = time.time()\n",
        "time_linear_train = t1-t0\n",
        "time_linear_predict = t2-t1\n",
        "# results\n",
        "print(\"Training time: %fs; Prediction time: %fs\" % (time_linear_train, time_linear_predict))\n",
        "report = classification_report(testData['compound'], prediction_linear, output_dict=True)\n",
        "print('positive: ', report['1'])\n",
        "print('negative: ', report['-1'])\n",
        "print('neutral: ',report['0'])"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training time: 795.632536s; Prediction time: 71.076929s\n",
            "positive:  {'precision': 0.9019192987758803, 'recall': 0.7051039697542533, 'f1-score': 0.7914594522909622, 'support': 8464}\n",
            "negative:  {'precision': 0.5790408525754884, 'recall': 0.44474761255115963, 'f1-score': 0.5030864197530864, 'support': 733}\n",
            "neutral:  {'precision': 0.8410774410774411, 'recall': 0.9484275137632096, 'f1-score': 0.8915325818636053, 'support': 15803}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8X8yLa5TcMju",
        "outputId": "af60df0e-69f3-4d13-9e56-569c3a8f21bc"
      },
      "source": [
        "review = \"\"\"I hate you\"\"\"\n",
        "review_vector = vectorizer.transform([review]) # vectorizing\n",
        "print(classifier_linear.predict(review_vector))"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['-1']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EZpR8W6bsN2G"
      },
      "source": [
        "from sklearn.metrics import  accuracy_score,confusion_matrix"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cS1u_M3Gshd6",
        "outputId": "98a53b27-6446-4d85-910c-8959178fe567"
      },
      "source": [
        "accuracy_score(testData['compound'],prediction_linear)"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.85128"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jV9luFfFsofP",
        "outputId": "25cb5f5f-1e21-4d5c-bc9f-10d340f0c711"
      },
      "source": [
        "confusion_matrix(testData['compound'],prediction_linear)"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[  326,   373,    34],\n",
              "       [  200, 14988,   615],\n",
              "       [   37,  2459,  5968]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eTqH_-dVszOJ"
      },
      "source": [
        "#saving the model\n",
        "\n",
        "import pickle\n",
        "# pickling the vectorizer\n",
        "pickle.dump(vectorizer, open('vectorizer.sav', 'wb'))\n",
        "# pickling the model\n",
        "pickle.dump(classifier_linear, open('classifier.sav', 'wb'))"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yH6m1HY2Ddse"
      },
      "source": [
        "!pip install streamlit --quiet\n",
        "!pip install pyngrok==4.1.1 --quiet\n",
        "from pyngrok import ngrok"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bpWZkB8xE-4A",
        "outputId": "8829e7ac-4bf4-48f7-bbdf-d21c3280e10b"
      },
      "source": [
        "%%writefile app.py\n",
        "import streamlit as st\n",
        "import pickle\n",
        "from sklearn import svm\n",
        "\n",
        "\n",
        "filename = 'classifier.sav'\n",
        "classifier_linear=pickle.load(open(filename,'rb'))\n",
        "filename1 = 'vectorizer.sav'\n",
        "vectorizer=pickle.load(open(filename1,'rb'))\n",
        "\n",
        "st.title('Sentiment Analysis using Machine Learning Algorithms')\n",
        "text=st.text_area('Enter text')\n",
        "\n",
        "st.set_option('deprecation.showfileUploaderEncoding', False)\n",
        "\n",
        "if st.button(\"Predict\"):\n",
        "  review_vector = vectorizer.transform([text])\n",
        "  x=classifier_linear.predict(review_vector)\n",
        "  if (x[0]=='0'):\n",
        "    st.success('Text is  NEUTRAL')\n",
        "  if (x[0]=='1'):\n",
        "    st.success('Text is POSITIVE')\n",
        "  if (x[0]=='-1'):\n",
        "    st.success(' Text is NEGATIVE')"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Overwriting app.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I5CW6aiOFEw5",
        "outputId": "6b4f0b39-5fc9-4a39-e425-ace01e2494a0"
      },
      "source": [
        "!nohup streamlit run app.py &\n",
        "url = ngrok.connect(port = '8501')\n",
        "print(url)"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "nohup: appending output to 'nohup.out'\n",
            "http://d268d581759d.ngrok.io\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "KUbT-XqJvd1_",
        "outputId": "388f8570-0107-46ba-c8a3-452a455ca671"
      },
      "source": [
        "import sys\n",
        "sys.version"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'3.7.10 (default, May  3 2021, 02:48:31) \\n[GCC 7.5.0]'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JpsbWWhoV8AP",
        "outputId": "4f97bba5-5b27-4073-8a7a-b14ab3dd2a4d"
      },
      "source": [
        "import pickle \n",
        "print(pickle.format_version)"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "4.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "05SE7YUuXyc9"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}